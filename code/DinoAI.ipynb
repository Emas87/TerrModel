{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Install and Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%pip install stable-baselines3[extra] protobuf==3.20.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%pip install mss pydirectinput pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#%pip install gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from mss import mss\n",
    "import pydirectinput\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pytesseract\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "#import gymnasium as gym\n",
    "#from gymnasium import spaces\n",
    "import gym\n",
    "from gym import spaces\n",
    "from TerrarianEyes import TerrarianEyes\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Build the Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Create Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "class TerrEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 4}\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Setup spaces\n",
    "        self.observation_space = spaces.Dict(\n",
    "            {\n",
    "                'map' :       spaces.Box(low=0, high=11, shape=(67, 120), dtype=np.int8),\n",
    "                'inventory' : spaces.Box(low=0, high=11, shape=(9,10), dtype=np.int8),\n",
    "            }\n",
    "        )\n",
    "        self.action_space = spaces.Discrete(7)\n",
    "        # Capture game frames\n",
    "        self.cap = mss()\n",
    "        self.game_location = {'top': 0, 'left': 0, 'width': 1920, 'height': 1080}\n",
    "        self.done_location = {'top': 460, 'left': 760, 'width': 400, 'height': 70}\n",
    "        self.action_map = {\n",
    "            0: 'no_op',\n",
    "            1: 'space',\n",
    "            2: 'w', \n",
    "            3: 'd', \n",
    "            4: 'a', \n",
    "            5: 'attack',\n",
    "            6: 'cut'\n",
    "        }\n",
    "        # Create Instance\n",
    "        tiles_weights_path = os.path.join('runs', 'train', 'yolov5s6-tiles', 'weights', 'best.pt')\n",
    "        objects_weights_path = os.path.join('runs', 'train', 'yolov5l6-objects', 'weights', 'best.pt')\n",
    "        self.eyes = TerrarianEyes(tiles_weights_path, objects_weights_path)\n",
    "        self.timer = None\n",
    "        self.time_limit = 120\n",
    "        self.day_timer = time.time()\n",
    "        self.day_limit = 360\n",
    "           \n",
    "    def step(self, action):\n",
    "        if self.timer is None:\n",
    "            raise AssertionError(\"Cannot call env.step() before calling reset()\")\n",
    "        # Get current health\n",
    "        health = self.eyes.map.getHealth()\n",
    "        if action == 0:\n",
    "            reward = 0\n",
    "        elif action < 5:\n",
    "            pydirectinput.press(self.action_map[action])\n",
    "            reward = 1 \n",
    "        else: \n",
    "            # In case we need map or inventory\n",
    "            if action == 5: # attack\n",
    "                # find closest enemy position and check \n",
    "                with open(\"delete.txt\", 'w') as f:\n",
    "                    f.write(str(self.eyes.map))\n",
    "                attack, x, y= self.eyes.map.isEnemyOnAttackRange()\n",
    "                # if is in attack range\n",
    "                if attack:\n",
    "                    # Move mouse to enemy position\n",
    "                    pydirectinput.moveTo((x+1)*16 + 16, y*16 + 8)\n",
    "                    # attack\n",
    "                    pydirectinput.press('1')\n",
    "                    # Press the left mouse button\n",
    "                    pydirectinput.mouseDown(button='left')\n",
    "                    # Release the left mouse button\n",
    "                    pydirectinput.mouseUp(button='left')\n",
    "                    reward = 2\n",
    "                else:\n",
    "                    # if not\n",
    "                    reward = 0\n",
    "            elif action == 6: # cut wood\n",
    "                # find closest tree position and check \n",
    "                # if is in cut range\n",
    "                cut, x, y = self.eyes.map.isTreeOnCutRange()\n",
    "                # if is in attack range\n",
    "                if cut:\n",
    "                    # Move mouse to enemy position\n",
    "                    pydirectinput.moveTo((x+1)*16 + 8, y*16 + 8)\n",
    "                    \n",
    "                    # attack\n",
    "                    pydirectinput.press('3')\n",
    "                    # Press the left mouse button\n",
    "                    pydirectinput.mouseDown(button='left')\n",
    "                    # Release the left mouse button\n",
    "                    pydirectinput.mouseUp(button='left')\n",
    "                    reward = 3\n",
    "                else:\n",
    "                    # if not\n",
    "                    reward = 0\n",
    "\n",
    "        done, _ = self.get_done() \n",
    "        observation = self.get_observation()\n",
    "        new_health = self.eyes.map.getHealth()\n",
    "        if new_health - health < 0:\n",
    "            reward = reward - 2\n",
    "        # calculate real reward\n",
    "        info = {}\n",
    "        return observation, reward, done, info\n",
    "        \n",
    "    \n",
    "    def reset(self):\n",
    "        time.sleep(10)\n",
    "        pydirectinput.click(x=150, y=250)\n",
    "        self.timer = time.time()\n",
    "        observation = self._get_obs()\n",
    "        return observation\n",
    "        \n",
    "    def render(self):\n",
    "        cv2.imshow('Game', self.current_frame)\n",
    "        key = cv2.waitKey(1)\n",
    "        if key == ord('q'):\n",
    "            self.close()\n",
    "         \n",
    "    def close(self):\n",
    "        cv2.destroyAllWindows()\n",
    "    \n",
    "    #def _get_obs(self):\n",
    "        #return {\"map\": self.eyes.map.current_map, \"inventory\": self.eyes.inventory.inventory}\n",
    "\n",
    "    def _get_obs(self):\n",
    "        return {\"map\": self.eyes.map.current_map, \"inventory\": self.eyes.inventory.inventory}\n",
    "\n",
    "    def get_observation(self):\n",
    "        raw = np.array(self.cap.grab(self.game_location))[:,:,:3].astype(np.uint8)\n",
    "        self.eyes.updateMap(raw)\n",
    "        self.eyes.updateInventory(raw)\n",
    "        #gray = cv2.cvtColor(raw, cv2.COLOR_BGR2GRAY)\n",
    "        #resized = cv2.resize(gray, (1920,1080))\n",
    "        #channel = np.reshape(resized, (1,1080,1920))\n",
    "        return {\"map\": self.eyes.map.current_map, \"inventory\": self.eyes.inventory.inventory}\n",
    "    \n",
    "    def get_done(self):\n",
    "        done = False\n",
    "        done_cap = None\n",
    "        if time.time() - self.day_timer  > self.day_limit:\n",
    "            # reset day\n",
    "\n",
    "            # Open inventory\n",
    "            pydirectinput.press('Esc')\n",
    "            # Click Power menu\n",
    "            pydirectinput.moveTo(50, 315)\n",
    "            # Press the left mouse button\n",
    "            pydirectinput.mouseDown(button='left')\n",
    "            # Release the left mouse button\n",
    "            pydirectinput.mouseUp(button='left')\n",
    "\n",
    "            #Click other place in the power menu to reset view\n",
    "            pydirectinput.moveTo(50, 530)\n",
    "            # Press the left mouse button\n",
    "            pydirectinput.mouseDown(button='left')\n",
    "            # Release the left mouse button\n",
    "            pydirectinput.mouseUp(button='left')\n",
    "\n",
    "            #Click time menu\n",
    "            pydirectinput.moveTo(50, 630)\n",
    "            # Press the left mouse button\n",
    "            pydirectinput.mouseDown(button='left')\n",
    "            # Release the left mouse button\n",
    "            pydirectinput.mouseUp(button='left')\n",
    "\n",
    "            #Click dawn\n",
    "            pydirectinput.moveTo(115, 655)\n",
    "            # Press the left mouse button\n",
    "            pydirectinput.mouseDown(button='left')\n",
    "            # Release the left mouse button\n",
    "            pydirectinput.mouseUp(button='left')\n",
    "\n",
    "            \n",
    "            # Close inventory\n",
    "            pydirectinput.press('Esc')\n",
    "\n",
    "            self.day_timer = time.time()            \n",
    "\n",
    "        elif time.time() - self.timer  > self.time_limit:\n",
    "            done = True\n",
    "        else:\n",
    "            done_cap = np.array(self.cap.grab(self.done_location))\n",
    "            done_strings = ['You', 'You ']\n",
    "            res = pytesseract.image_to_string(done_cap)[:4]\n",
    "            if res in done_strings:\n",
    "                done = True\n",
    "        return done, done_cap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Test Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5  2023-3-26 Python-3.9.2rc1 torch-1.13.1+cu116 CUDA:0 (NVIDIA GeForce RTX 3070 Ti, 8192MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 206 layers, 12319756 parameters, 0 gradients, 16.1 GFLOPs\n",
      "Fusing layers... \n",
      "Model summary: 346 layers, 76157124 parameters, 0 gradients, 110.0 GFLOPs\n"
     ]
    }
   ],
   "source": [
    "env = TerrEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#obs=env.get_observation()\n",
    "#env.eyes.updateMap(env.observation)\n",
    "#env.eyes.map.getHealth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.imshow(cv2.cvtColor(obs[0], cv2.COLOR_GRAY2BGR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "#done, done_cap = env.get_done()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.imshow(done_cap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pytesseract.image_to_string(done_cap)[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[327], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m done:\n\u001b[0;32m      6\u001b[0m     action \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39msample()\n\u001b[1;32m----> 7\u001b[0m     obs, reward,  done, info \u001b[39m=\u001b[39m  env\u001b[39m.\u001b[39;49mstep(env\u001b[39m.\u001b[39;49maction_space\u001b[39m.\u001b[39;49msample())\n\u001b[0;32m      8\u001b[0m     total_reward  \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m reward\n\u001b[0;32m      9\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mTotal Reward for episode \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m is \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(episode, total_reward))    \n",
      "Cell \u001b[1;32mIn[320], line 45\u001b[0m, in \u001b[0;36mTerrEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     43\u001b[0m     reward \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     44\u001b[0m \u001b[39melif\u001b[39;00m action \u001b[39m<\u001b[39m \u001b[39m5\u001b[39m:\n\u001b[1;32m---> 45\u001b[0m     pydirectinput\u001b[39m.\u001b[39;49mpress(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maction_map[action])\n\u001b[0;32m     46\u001b[0m     reward \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \n\u001b[0;32m     47\u001b[0m \u001b[39melse\u001b[39;00m: \n\u001b[0;32m     48\u001b[0m     \u001b[39m# In case we need map or inventory\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\pydirectinput\\__init__.py:242\u001b[0m, in \u001b[0;36m_genericPyDirectInputChecks.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    239\u001b[0m funcArgs \u001b[39m=\u001b[39m inspect\u001b[39m.\u001b[39mgetcallargs(wrappedFunction, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    241\u001b[0m failSafeCheck()\n\u001b[1;32m--> 242\u001b[0m returnVal \u001b[39m=\u001b[39m wrappedFunction(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    243\u001b[0m _handlePause(funcArgs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39m_pause\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m    244\u001b[0m \u001b[39mreturn\u001b[39;00m returnVal\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\pydirectinput\\__init__.py:544\u001b[0m, in \u001b[0;36mpress\u001b[1;34m(keys, presses, interval, logScreenshot, _pause)\u001b[0m\n\u001b[0;32m    542\u001b[0m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m keys:\n\u001b[0;32m    543\u001b[0m     failSafeCheck()\n\u001b[1;32m--> 544\u001b[0m     downed \u001b[39m=\u001b[39m keyDown(k)\n\u001b[0;32m    545\u001b[0m     upped \u001b[39m=\u001b[39m keyUp(k)\n\u001b[0;32m    546\u001b[0m     \u001b[39m# Count key press as complete if key was \"downed\" and \"upped\" successfully\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\pydirectinput\\__init__.py:243\u001b[0m, in \u001b[0;36m_genericPyDirectInputChecks.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    241\u001b[0m failSafeCheck()\n\u001b[0;32m    242\u001b[0m returnVal \u001b[39m=\u001b[39m wrappedFunction(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 243\u001b[0m _handlePause(funcArgs\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39m_pause\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[0;32m    244\u001b[0m \u001b[39mreturn\u001b[39;00m returnVal\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\pydirectinput\\__init__.py:232\u001b[0m, in \u001b[0;36m_handlePause\u001b[1;34m(_pause)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[39mif\u001b[39;00m _pause:\n\u001b[0;32m    231\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(PAUSE, \u001b[39mint\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(PAUSE, \u001b[39mfloat\u001b[39m)\n\u001b[1;32m--> 232\u001b[0m     time\u001b[39m.\u001b[39;49msleep(PAUSE)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for episode in range(10): \n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward,  done, info =  env.step(env.action_space.sample())\n",
    "        total_reward  += reward\n",
    "    print('Total Reward for episode {} is {}'.format(episode, total_reward))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Create Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import os for file path management\n",
    "import os \n",
    "# Import Base Callback for saving models\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "# Check Environment    \n",
    "from stable_baselines3.common import env_checker\n",
    "from gym.utils.env_checker import check_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env_checker.check_env(env)\n",
    "check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = './train/'\n",
    "LOG_DIR = './logs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = TrainAndLoggingCallback(check_freq=1000, save_path=CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Build DQN and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TerrEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "model = DQN('MultiInputPolicy', env, tensorboard_log=LOG_DIR, verbose=1, buffer_size=1200000, learning_starts=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./logs/DQN_1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[300], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model\u001b[39m.\u001b[39;49mlearn(total_timesteps\u001b[39m=\u001b[39;49m\u001b[39m100000\u001b[39;49m, callback\u001b[39m=\u001b[39;49mcallback)\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\stable_baselines3\\dqn\\dqn.py:269\u001b[0m, in \u001b[0;36mDQN.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[0;32m    261\u001b[0m     \u001b[39mself\u001b[39m: SelfDQN,\n\u001b[0;32m    262\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    267\u001b[0m     progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    268\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m SelfDQN:\n\u001b[1;32m--> 269\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mlearn(\n\u001b[0;32m    270\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[0;32m    271\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[0;32m    272\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[0;32m    273\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[0;32m    274\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[0;32m    275\u001b[0m         progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[0;32m    276\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:311\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    308\u001b[0m callback\u001b[39m.\u001b[39mon_training_start(\u001b[39mlocals\u001b[39m(), \u001b[39mglobals\u001b[39m())\n\u001b[0;32m    310\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m<\u001b[39m total_timesteps:\n\u001b[1;32m--> 311\u001b[0m     rollout \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollect_rollouts(\n\u001b[0;32m    312\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv,\n\u001b[0;32m    313\u001b[0m         train_freq\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_freq,\n\u001b[0;32m    314\u001b[0m         action_noise\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maction_noise,\n\u001b[0;32m    315\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[0;32m    316\u001b[0m         learning_starts\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlearning_starts,\n\u001b[0;32m    317\u001b[0m         replay_buffer\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreplay_buffer,\n\u001b[0;32m    318\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[0;32m    319\u001b[0m     )\n\u001b[0;32m    321\u001b[0m     \u001b[39mif\u001b[39;00m rollout\u001b[39m.\u001b[39mcontinue_training \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[0;32m    322\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:543\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.collect_rollouts\u001b[1;34m(self, env, callback, train_freq, replay_buffer, action_noise, learning_starts, log_interval)\u001b[0m\n\u001b[0;32m    540\u001b[0m actions, buffer_actions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sample_action(learning_starts, action_noise, env\u001b[39m.\u001b[39mnum_envs)\n\u001b[0;32m    542\u001b[0m \u001b[39m# Rescale and perform action\u001b[39;00m\n\u001b[1;32m--> 543\u001b[0m new_obs, rewards, dones, infos \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(actions)\n\u001b[0;32m    545\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mnum_envs\n\u001b[0;32m    546\u001b[0m num_collected_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:163\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \u001b[39mStep the environments with the given action\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \n\u001b[0;32m    159\u001b[0m \u001b[39m:param actions: the action\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[39m:return: observation, reward, done, information\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_async(actions)\n\u001b[1;32m--> 163\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep_wait()\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:54\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep_wait\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m VecEnvStepReturn:\n\u001b[0;32m     53\u001b[0m     \u001b[39mfor\u001b[39;00m env_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_envs):\n\u001b[1;32m---> 54\u001b[0m         obs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_rews[env_idx], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_dones[env_idx], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_infos[env_idx] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menvs[env_idx]\u001b[39m.\u001b[39;49mstep(\n\u001b[0;32m     55\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactions[env_idx]\n\u001b[0;32m     56\u001b[0m         )\n\u001b[0;32m     57\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_dones[env_idx]:\n\u001b[0;32m     58\u001b[0m             \u001b[39m# save final observation where user can get it, then reset\u001b[39;00m\n\u001b[0;32m     59\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_infos[env_idx][\u001b[39m\"\u001b[39m\u001b[39mterminal_observation\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m obs\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\stable_baselines3\\common\\monitor.py:95\u001b[0m, in \u001b[0;36mMonitor.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mneeds_reset:\n\u001b[0;32m     94\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTried to step environment that needs reset\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 95\u001b[0m observation, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m     96\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrewards\u001b[39m.\u001b[39mappend(reward)\n\u001b[0;32m     97\u001b[0m \u001b[39mif\u001b[39;00m done:\n",
      "Cell \u001b[1;32mIn[284], line 45\u001b[0m, in \u001b[0;36mTerrEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     43\u001b[0m     reward \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     44\u001b[0m \u001b[39melif\u001b[39;00m action \u001b[39m<\u001b[39m \u001b[39m5\u001b[39m:\n\u001b[1;32m---> 45\u001b[0m     pydirectinput\u001b[39m.\u001b[39;49mpress(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maction_map[action])\n\u001b[0;32m     46\u001b[0m     reward \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \n\u001b[0;32m     47\u001b[0m \u001b[39melse\u001b[39;00m: \n\u001b[0;32m     48\u001b[0m     \u001b[39m# In case we need map or inventory\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\pydirectinput\\__init__.py:242\u001b[0m, in \u001b[0;36m_genericPyDirectInputChecks.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    239\u001b[0m funcArgs \u001b[39m=\u001b[39m inspect\u001b[39m.\u001b[39mgetcallargs(wrappedFunction, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    241\u001b[0m failSafeCheck()\n\u001b[1;32m--> 242\u001b[0m returnVal \u001b[39m=\u001b[39m wrappedFunction(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    243\u001b[0m _handlePause(funcArgs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39m_pause\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m    244\u001b[0m \u001b[39mreturn\u001b[39;00m returnVal\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\pydirectinput\\__init__.py:544\u001b[0m, in \u001b[0;36mpress\u001b[1;34m(keys, presses, interval, logScreenshot, _pause)\u001b[0m\n\u001b[0;32m    542\u001b[0m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m keys:\n\u001b[0;32m    543\u001b[0m     failSafeCheck()\n\u001b[1;32m--> 544\u001b[0m     downed \u001b[39m=\u001b[39m keyDown(k)\n\u001b[0;32m    545\u001b[0m     upped \u001b[39m=\u001b[39m keyUp(k)\n\u001b[0;32m    546\u001b[0m     \u001b[39m# Count key press as complete if key was \"downed\" and \"upped\" successfully\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\pydirectinput\\__init__.py:243\u001b[0m, in \u001b[0;36m_genericPyDirectInputChecks.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    241\u001b[0m failSafeCheck()\n\u001b[0;32m    242\u001b[0m returnVal \u001b[39m=\u001b[39m wrappedFunction(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 243\u001b[0m _handlePause(funcArgs\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39m_pause\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[0;32m    244\u001b[0m \u001b[39mreturn\u001b[39;00m returnVal\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\pydirectinput\\__init__.py:232\u001b[0m, in \u001b[0;36m_handlePause\u001b[1;34m(_pause)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[39mif\u001b[39;00m _pause:\n\u001b[0;32m    231\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(PAUSE, \u001b[39mint\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(PAUSE, \u001b[39mfloat\u001b[39m)\n\u001b[1;32m--> 232\u001b[0m     time\u001b[39m.\u001b[39;49msleep(PAUSE)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.learn(total_timesteps=100000, callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load('train_first/best_mode l_50000') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Test out Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for episode in range(5): \n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done: \n",
    "        action, findOut = model.predict(obs)\n",
    "        obs, reward, done, info = env.step(int(action))\n",
    "        time.sleep(0.01)\n",
    "        total_reward += reward\n",
    "    print('Total Reward for episode {} is {}'.format(episode, total_reward))\n",
    "    time.sleep(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
