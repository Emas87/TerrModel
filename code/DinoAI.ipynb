{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Install and Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%pip install stable-baselines3[extra] protobuf==3.20.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%pip install mss pydirectinput pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#%pip install gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from mss import mss\n",
    "import pydirectinput\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pytesseract\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "from gymnasium import Env\n",
    "from gymnasium.spaces import Box, Discrete\n",
    "from TerrarianEyes import TerrarianEyes\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Build the Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Create Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "class TerrEnv(Env):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Setup spaces\n",
    "        self.observation_space = Box(low=0, high=255, shape=(1,1080,1920), dtype=np.uint8)\n",
    "        self.action_space = Discrete(8)\n",
    "        # Capture game frames\n",
    "        self.cap = mss()\n",
    "        self.game_location = {'top': 0, 'left': 0, 'width': 1920, 'height': 1080}\n",
    "        self.done_location = {'top': 460, 'left': 760, 'width': 400, 'height': 70}\n",
    "        self.action_map = {\n",
    "            0: 'no_op',\n",
    "            1: 'space',\n",
    "            2: 'w', \n",
    "            3: 'd', \n",
    "            4: 'a', \n",
    "            5: 'h', # heal\n",
    "            6: 'attack',\n",
    "            7: 'cut',\n",
    "            #8: 'mine up',\n",
    "            #9: 'mine down',\n",
    "            #10: 'mine left',\n",
    "            #11: 'mine right',\n",
    "            #12: 'attack bow',\n",
    "            #13: 'use torch',\n",
    "            #14: 'use rope',\n",
    "            #15: 'Esc', # open inventory\n",
    "        }\n",
    "        # Create Instance\n",
    "        tiles_weights_path = os.path.join('runs', 'train', 'yolov5l6-tiles', 'weights', 'best.pt')\n",
    "        objects_weights_path = os.path.join('runs', 'train', 'yolov5l6-objects', 'weights', 'best.pt')\n",
    "        self.eyes = TerrarianEyes(tiles_weights_path, objects_weights_path)\n",
    "        self.observation = None\n",
    "        self.timer = None\n",
    "        self.time_limit = 120\n",
    "           \n",
    "    def step(self, action):\n",
    "        self.eyes.updateMap(self.observation)\n",
    "        # Get current health\n",
    "        health = self.eyes.map.getHealth()\n",
    "        if action == 0:\n",
    "            reward = 0\n",
    "        elif action < 6:\n",
    "            pydirectinput.press(self.action_map[action])\n",
    "            reward = 1 \n",
    "        else: \n",
    "            self.eyes.updateInventory(self.observation)\n",
    "            # In case we need map or inventory\n",
    "            if action == 6: # attack\n",
    "                # find closest enemy position and check \n",
    "                with open(\"delete.txt\", 'w') as f:\n",
    "                    f.write(str(self.eyes.map))\n",
    "                attack, x, y= self.eyes.map.isEnemyOnAttackRange()\n",
    "                # if is in attack range\n",
    "                if attack:\n",
    "                    # Move mouse to enemy position\n",
    "                    pydirectinput.moveTo((x+1)*16 + 16, y*16 + 8)\n",
    "                    # attack\n",
    "                    pydirectinput.press('1')\n",
    "                    # Press the left mouse button\n",
    "                    pydirectinput.mouseDown(button='left')\n",
    "                    # Release the left mouse button\n",
    "                    pydirectinput.mouseUp(button='left')\n",
    "                    reward = 2\n",
    "                else:\n",
    "                    # if not\n",
    "                    reward = 0\n",
    "            elif action == 7: # cut wood\n",
    "                # find closest tree position and check \n",
    "                # if is in cut range\n",
    "                cut, x, y = self.eyes.map.isTreeOnCutRange()\n",
    "                # if is in attack range\n",
    "                if cut:\n",
    "                    # Move mouse to enemy position\n",
    "                    pydirectinput.moveTo((x+1)*16 + 8, y*16 + 8)\n",
    "                    \n",
    "                    # attack\n",
    "                    pydirectinput.press('3')\n",
    "                    # Press the left mouse button\n",
    "                    pydirectinput.mouseDown(button='left')\n",
    "                    # Release the left mouse button\n",
    "                    pydirectinput.mouseUp(button='left')\n",
    "                    reward = 3\n",
    "                else:\n",
    "                    # if not\n",
    "                    reward = 0\n",
    "\n",
    "            elif action == 8: # mine up FUTURE WORK\n",
    "                # Move mouse above player position\n",
    "                pydirectinput.press(self.action_map[2])\n",
    "                # Check what was mined\n",
    "                reward = 2\n",
    "            elif action == 9: # mine down FUTURE WORK\n",
    "                # Move mouse below player position\n",
    "                pydirectinput.press(self.action_map[2])\n",
    "                # Check what was mined\n",
    "                reward = 2\n",
    "            elif action == 10: # mine left FUTURE WORK\n",
    "                # Move mouse left to player position\n",
    "                pydirectinput.press(self.action_map[2])\n",
    "                # Check what was mined\n",
    "                reward = 2\n",
    "            elif action == 11: # mine right FUTURE WORK\n",
    "                # Move mouse right to player position\n",
    "                pydirectinput.press(self.action_map[2])\n",
    "                # Check what was mined\n",
    "                reward = 2\n",
    "            elif action == 12:\n",
    "                # Find bow and use it in closest enemy FUTURE WORK\n",
    "                pydirectinput.press(self.action_map[4])\n",
    "            elif action == 13:\n",
    "                # Find torch and use it FUTURE WORK\n",
    "                pydirectinput.press(self.action_map[5])\n",
    "            elif action == 14:\n",
    "                # Find rope and use it FUTURE WORK\n",
    "                pydirectinput.press(self.action_map[6])\n",
    "\n",
    "        done, _ = self.get_done() \n",
    "        observation = self.get_observation()\n",
    "        # calculate real reward\n",
    "        info = {}\n",
    "        return observation, reward, done, info\n",
    "        \n",
    "    \n",
    "    def reset(self):\n",
    "        time.sleep(10)\n",
    "        pydirectinput.click(x=150, y=150)\n",
    "        self.timer = time.time()\n",
    "        return self.get_observation()\n",
    "        \n",
    "    def render(self):\n",
    "        cv2.imshow('Game', self.current_frame)\n",
    "        key = cv2.waitKey(1)\n",
    "        if key == ord('q'):\n",
    "            self.close()\n",
    "         \n",
    "    def close(self):\n",
    "        cv2.destroyAllWindows()\n",
    "    \n",
    "    def get_observation(self):\n",
    "        raw = np.array(self.cap.grab(self.game_location))[:,:,:3].astype(np.uint8)\n",
    "        self.observation = raw\n",
    "        gray = cv2.cvtColor(raw, cv2.COLOR_BGR2GRAY)\n",
    "        resized = cv2.resize(gray, (1920,1080))\n",
    "        channel = np.reshape(resized, (1,1080,1920))\n",
    "        return channel\n",
    "    \n",
    "    def get_done(self):\n",
    "        done = False\n",
    "        done_cap = None\n",
    "        if self.timer - time.time() > self.time_limit:\n",
    "            done = True\n",
    "        else:\n",
    "            done_cap = np.array(self.cap.grab(self.done_location))\n",
    "            done_strings = ['You', 'You ']\n",
    "            res = pytesseract.image_to_string(done_cap)[:4]\n",
    "            if res in done_strings:\n",
    "                done = True\n",
    "        return done, done_cap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Test Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5  2023-3-26 Python-3.9.2rc1 torch-1.13.1+cu116 CUDA:0 (NVIDIA GeForce RTX 3070 Ti, 8192MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 346 layers, 76164816 parameters, 0 gradients, 110.0 GFLOPs\n",
      "Fusing layers... \n",
      "Model summary: 346 layers, 76326348 parameters, 0 gradients, 110.3 GFLOPs\n"
     ]
    }
   ],
   "source": [
    "env = TerrEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#obs=env.get_observation()\n",
    "#env.eyes.updateMap(env.observation)\n",
    "#env.eyes.map.getHealth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.imshow(cv2.cvtColor(obs[0], cv2.COLOR_GRAY2BGR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#done, done_cap = env.get_done()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.imshow(done_cap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pytesseract.image_to_string(done_cap)[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m done:\n\u001b[0;32m      6\u001b[0m     action \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39msample()\n\u001b[1;32m----> 7\u001b[0m     obs, reward,  done, info \u001b[39m=\u001b[39m  env\u001b[39m.\u001b[39;49mstep(env\u001b[39m.\u001b[39;49maction_space\u001b[39m.\u001b[39;49msample())\n\u001b[0;32m      8\u001b[0m     total_reward  \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m reward\n\u001b[0;32m      9\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mTotal Reward for episode \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m is \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(episode, total_reward))    \n",
      "Cell \u001b[1;32mIn[20], line 46\u001b[0m, in \u001b[0;36mTerrEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     44\u001b[0m     reward \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     45\u001b[0m \u001b[39melif\u001b[39;00m action \u001b[39m<\u001b[39m \u001b[39m6\u001b[39m:\n\u001b[1;32m---> 46\u001b[0m     pydirectinput\u001b[39m.\u001b[39;49mpress(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maction_map[action])\n\u001b[0;32m     47\u001b[0m     reward \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \n\u001b[0;32m     48\u001b[0m \u001b[39melse\u001b[39;00m: \n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\pydirectinput\\__init__.py:242\u001b[0m, in \u001b[0;36m_genericPyDirectInputChecks.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    239\u001b[0m funcArgs \u001b[39m=\u001b[39m inspect\u001b[39m.\u001b[39mgetcallargs(wrappedFunction, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    241\u001b[0m failSafeCheck()\n\u001b[1;32m--> 242\u001b[0m returnVal \u001b[39m=\u001b[39m wrappedFunction(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    243\u001b[0m _handlePause(funcArgs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39m_pause\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m    244\u001b[0m \u001b[39mreturn\u001b[39;00m returnVal\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\pydirectinput\\__init__.py:545\u001b[0m, in \u001b[0;36mpress\u001b[1;34m(keys, presses, interval, logScreenshot, _pause)\u001b[0m\n\u001b[0;32m    543\u001b[0m failSafeCheck()\n\u001b[0;32m    544\u001b[0m downed \u001b[39m=\u001b[39m keyDown(k)\n\u001b[1;32m--> 545\u001b[0m upped \u001b[39m=\u001b[39m keyUp(k)\n\u001b[0;32m    546\u001b[0m \u001b[39m# Count key press as complete if key was \"downed\" and \"upped\" successfully\u001b[39;00m\n\u001b[0;32m    547\u001b[0m \u001b[39mif\u001b[39;00m downed \u001b[39mand\u001b[39;00m upped:\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\pydirectinput\\__init__.py:243\u001b[0m, in \u001b[0;36m_genericPyDirectInputChecks.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    241\u001b[0m failSafeCheck()\n\u001b[0;32m    242\u001b[0m returnVal \u001b[39m=\u001b[39m wrappedFunction(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 243\u001b[0m _handlePause(funcArgs\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39m_pause\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[0;32m    244\u001b[0m \u001b[39mreturn\u001b[39;00m returnVal\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\pydirectinput\\__init__.py:232\u001b[0m, in \u001b[0;36m_handlePause\u001b[1;34m(_pause)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[39mif\u001b[39;00m _pause:\n\u001b[0;32m    231\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(PAUSE, \u001b[39mint\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(PAUSE, \u001b[39mfloat\u001b[39m)\n\u001b[1;32m--> 232\u001b[0m     time\u001b[39m.\u001b[39;49msleep(PAUSE)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for episode in range(10): \n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward,  done, info =  env.step(env.action_space.sample())\n",
    "        total_reward  += reward\n",
    "    print('Total Reward for episode {} is {}'.format(episode, total_reward))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Create Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import os for file path management\n",
    "import os \n",
    "# Import Base Callback for saving models\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "# Check Environment    \n",
    "from stable_baselines3.common import env_checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_checker.check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = './train/'\n",
    "LOG_DIR = './logs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = TrainAndLoggingCallback(check_freq=1000, save_path=CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Build DQN and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TerrEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = DQN('CnnPolicy', env, tensorboard_log=LOG_DIR, verbose=1, buffer_size=1200000, learning_starts=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=100000, callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load('train_first/best_mode l_50000') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Test out Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for episode in range(5): \n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done: \n",
    "        action, findOut = model.predict(obs)\n",
    "        obs, reward, done, info = env.step(int(action))\n",
    "        time.sleep(0.01)\n",
    "        total_reward += reward\n",
    "    print('Total Reward for episode {} is {}'.format(episode, total_reward))\n",
    "    time.sleep(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
